---
title: "NALU gradient"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Defining NALU

$$
\begin{aligned}
W_{h_\ell, h_{\ell-1}} &= \tanh(\hat{W}_{h_\ell, h_{\ell-1}}) \sigma(\hat{M}_{h_\ell, h_{\ell-1}}) \\
a_{h_\ell} &= \sum_{h_{\ell-1}=1}^{H_{\ell-1}} W_{h_{\ell}, h_{\ell-1}} z_{h_{\ell-1}} \\
m_{h_\ell} &= \exp\left(\sum_{h_{\ell-1}=1}^{H_{\ell-1}} W_{h_{\ell}, h_{\ell-1}} \log(|z_{h_{\ell-1}}| + \epsilon) \right) \\
g_{h_\ell} &= \sigma(\hat{g}_{h_\ell}),\ \hat{g}_{h_\ell} = \sum_{h_{\ell-1}=1}^{H_{\ell-1}} G_{h_{\ell}, h_{\ell-1}} z_{h_{\ell-1}} \\
z_{h_\ell} &= g_{h_\ell} a_{h_\ell} + (1 - g_{h_\ell}) m_{h_\ell} \\
\mathcal{L} &= \sum_{h_{L}=1}^{H_L} (z_{h_L} - t_{h_L})^2
\end{aligned}
$$

# Deriving the gradients

## Weight gradient

### The NAC

$$
\begin{aligned}
\frac{\partial\mathcal{L}}{\partial \hat{W}_{h_\ell, h_{\ell-1}}} &= \frac{\partial\mathcal{L}}{\partial W_{h_\ell, h_{\ell-1}}} \frac{\partial W_{h_\ell, h_{\ell-1}}}{\partial \hat{W}_{h_\ell, h_{\ell-1}}} = \frac{\partial\mathcal{L}}{\partial W_{h_\ell, h_{\ell-1}}} (1 - \tanh^2(\hat{W}_{h_\ell, h_{\ell-1}})) \sigma(\hat{M}_{h_\ell, h_{\ell-1}}) \\
\frac{\partial\mathcal{L}}{\partial \hat{M}_{h_\ell, h_{\ell-1}}} &= \frac{\partial\mathcal{L}}{\partial W_{h_\ell, h_{\ell-1}}} \frac{\partial M_{h_\ell, h_{\ell-1}}}{\partial \hat{M}_{h_\ell, h_{\ell-1}}} = \frac{\partial\mathcal{L}}{\partial W_{h_\ell, h_{\ell-1}}} \tanh(\hat{W}_{h_\ell, h_{\ell-1}}) \sigma(\hat{M}_{h_\ell, h_{\ell-1}}) (1 - \sigma(\hat{M}_{h_\ell, h_{\ell-1}}))
\end{aligned}
$$

We see again, like with just the NAC, that how the "NAC" part is constructed doesn't matter for the overall NALU. A linear layer could also be used, and it would have no futher consequences for the NALU gradients.

### The NALU operations

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W_{h_\ell, h_{\ell-1}}}
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} \\
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \left(\frac{\partial z_{h_\ell}}{\partial a_{h_\ell}} \frac{\partial a_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} + \frac{\partial z_{h_\ell}}{\partial m_{h_\ell}} \frac{\partial m_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} \right) \\
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \left(g_{h_\ell} \frac{\partial a_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} + (1 - g_{h_\ell}) \frac{\partial m_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} \right) \\
&= \delta_{h\ell} \left(g_{h_\ell} \frac{\partial a_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} + (1 - g_{h_\ell}) \frac{\partial m_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} \right)
\end{aligned}
$$

Here we notice that the gate have the wanted effect on the gradient, as in it optimizes the $W_{h\ell, h_{\ell-1}}$ dependening on the gate. Although if the gradient is really bad this means that no futher optimization will occurre on $W_{h\ell, h_{\ell-1}}$ with respect to the correct operation, at least until the gate is correct.

One important thing, to think about. Is that if gates at some point attain a bad value, then this dependency will slow down convergence significantly.

$$
\begin{aligned}
\frac{\partial a_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} &= z_{h_{\ell-1}} \\
\frac{\partial m_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} &= \exp\left(\sum_{h'_{\ell-1}=1}^{H_{\ell-1}} W_{h_{\ell}, h'_{\ell-1}} \log(|z_{h'_{\ell-1}}| + \epsilon) \right) \log(|z_{h_{\ell-1}}| + \epsilon)
\end{aligned}
$$

Here one should wanter if such a large or small factor on $\log(|x_{h_{\ell-1}}| + \epsilon)$ is healfy for the gradient. An analog to $\frac{\partial a_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} = x_{h_{\ell-1}}$, would be:

$$
\frac{\partial b_{h_\ell}}{\partial W_{h_\ell, h_{\ell-1}}} \propto \log(|z_{h_{\ell-1}}| + \epsilon)
$$

### The NALU gate

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial G_{h_\ell, h_{\ell-1}}}
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial G_{h_\ell, h_{\ell-1}}} \\
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial g_{h_\ell}} \frac{\partial g_{h_\ell}}{\partial G_{h_\ell, h_{\ell-1}}} \\
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} (a_{h_\ell} - m_{h_\ell}) \frac{\partial g_{h_\ell}}{\partial G_{h_\ell, h_{\ell-1}}} \\
&= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} (a_{h_\ell} - m_{h_\ell}) \sigma(\hat{g}_{h_\ell}) (1 - \sigma(\hat{g}_{h_\ell})) \\
&= \delta_{h_\ell} (a_{h_\ell} - m_{h_\ell}) \sigma(\hat{g}_{h_\ell}) (1 - \sigma(\hat{g}_{h_\ell}))
\end{aligned}
$$

As $\sigma(\hat{g}_{h_\ell}) (1 - \sigma(\hat{g}_{h_\ell}))$ is always positive, $\frac{\partial \mathcal{L}}{\partial z_{h_\ell}} (a_{h_\ell} - b_{h_\ell})$ is the only part that controls the sign. The rest just becomes part of the learning rate. Thus we have:

$$
\frac{\partial \mathcal{L}}{\partial G_{h_\ell, h_{\ell-1}}} \propto \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} (a_{h_\ell} - m_{h_\ell})
$$

The intrepeation of this, is that if one has an underestimate ($\frac{\partial \mathcal{L}}{\partial G_{h_\ell, h_{\ell-1}}} < 0$), then gate converges to side with the largest value. And alternative if one has an overestimate($\frac{\partial \mathcal{L}}{\partial G_{h_\ell, h_{\ell-1}}} > 0$), then the gate converges to the side with the smallest value.

### Improvements to consider

#### The operation gradients

If completly seperate weight matrices are used for the operations, one have:

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W^a_{h_\ell, h_{\ell-1}}} &= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial W^a_{h_\ell, h_{\ell-1}}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial a_{h_\ell}} \frac{\partial a_{h_\ell}}{\partial W^a_{h_\ell, h_{\ell-1}}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} g_{h_\ell} \frac{\partial a_{h_\ell}}{\partial W^a_{h_\ell, h_{\ell-1}}} \\
\frac{\partial \mathcal{L}}{\partial W^b_{h_\ell, h_{\ell-1}}} &= \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial W^b_{h_\ell, h_{\ell-1}}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial b_{h_\ell}} \frac{\partial b_{h_\ell}}{\partial W^b_{h_\ell, h_{\ell-1}}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} (1 - g_{h_\ell}) \frac{\partial b_{h_\ell}}{\partial W^b_{h_\ell, h_{\ell-1}}}
\end{aligned}
$$

Of course, one could improve exploration by using:

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W^a_{h_\ell, h_{\ell-1}}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial a_{h_\ell}}{\partial W^a_{h_\ell, h_{\ell-1}}} \\
\frac{\partial \mathcal{L}}{\partial W^b_{h_\ell, h_{\ell-1}}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial b_{h_\ell}}{\partial W^b_{h_\ell, h_{\ell-1}}}
\end{aligned}
$$

However, this does not work when considering a problem that requires an input-dependent gate.

#### Decoupled operations

An interresting effect in when decoupling the operations with different matrices, is that the operations could also be decouples to be different networks.

$$
\begin{aligned}
g_{h_\ell} &= \sigma(z^g_{h_{\ell-1}}) \\
z_{h_\ell} &= g_{h_\ell} z^a_{h_{\ell-1}} + (1 - g_{h_\ell}) z^{m}_{h_{\ell-1}}
\end{aligned}
$$

### The gate gradients

One could perhaps improve exploration of the gate, by defining the gradient as:

$$
\frac{\partial \mathcal{L}}{\partial G_{h_\ell, h_{\ell-1}}} =  \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} (a_{h_\ell} - m_{h_\ell})
$$

The disadvantage here, is that the gradient is now decoupled from the $h_{\ell-1}$ index, thus this will only work for a bias controled gate. And that is perhaps not as interresting.


## Backpropergation term 

We now wish to derive the backpropergation term $\delta_{h_\ell} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}}$, because $z_{h_\ell}$ affects $\{z_{h_{\ell+1}}\}_{h_{\ell+1}=1}^{H_{\ell+1}}$ this becomes:
$$
\delta_{h_\ell} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} = \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \frac{\partial \mathcal{L}}{\partial z_{h_{\ell+1}}} \frac{\partial z_{h_{\ell+1}}}{\partial z_{h_\ell}} = \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \delta_{h_{\ell+1}} \frac{\partial z_{h_{\ell+1}}}{\partial z_{h_\ell}}
$$

To make it easier to derive $\frac{\partial z_{h_{\ell+1}}}{\partial z_{h_\ell}}$ we re-express the $z_{h_\ell}$ as  $z_{h_{\ell+1}}$.

$$
\begin{aligned}
a_{h_{\ell+1}} &= \sum_{h_{\ell}=1}^{H_\ell} W_{h_{\ell+1}, h_{\ell}} z_{h_{\ell}} \\
m_{h_{\ell+1}} &= \exp\left(\sum_{h_{\ell}=1}^{H_{\ell}} W_{h_{\ell+1}, h_{\ell}} \log(|z_{h_{\ell}}| + \epsilon) \right) \\
g_{h_{\ell+1}} &= \sigma(\hat{g}_{h_{\ell+1}}),\ \hat{g}_{h_{\ell+1}} = \sum_{h_{\ell}=1}^{H_{\ell}} G_{h_{\ell+1}, h_{\ell}} z_{h_{\ell}} \\
z_{h_{\ell+1}} &= g_{h_{\ell+1}} a_{h_{\ell+1}} + (1 - g_{h_{\ell+1}}) m_{h_{\ell+1}} \\
\end{aligned}
$$

### The NALU operation

$$
\begin{aligned}
\frac{\partial z_{h_{\ell+1}}}{\partial z_{h_\ell}} &= \frac{\partial z_{h_{\ell+1}}}{\partial a_{h_{\ell+1}}} \frac{\partial a_{h_{\ell+1}}}{\partial z_{h_\ell}} + \frac{\partial z_{h_{\ell+1}}}{\partial m_{h_{\ell+1}}} \frac{\partial m_{h_{\ell+1}}}{\partial z_{h_\ell}} + \frac{\partial z_{h_{\ell+1}}}{\partial g_{h_{\ell+1}}} \frac{\partial g_{h_{\ell+1}}}{\partial z_{h_\ell}} \\
&= g_{h_{\ell+1}} \frac{\partial a_{h_{\ell+1}}}{\partial z_{h_\ell}} + (1 - g_{h_{\ell+1}}) \frac{\partial m_{h_{\ell+1}}}{\partial z_{h_\ell}} + (a_{h_{\ell+1}} - m_{h_{\ell+1}}) \frac{\partial g_{h_{\ell+1}}}{\partial z_{h_\ell}}
\end{aligned}
$$

### The addition operation

$$
\frac{\partial a_{h_{\ell+1}}}{\partial z_{h_\ell}} = W_{h_{\ell+1}, h_{\ell}}
$$

### The multiplication operation

$$
\frac{\partial m_{h_{\ell+1}}}{\partial z_{h_\ell}} = \exp\left(\sum_{h_{\ell}=1}^{H_{\ell}} W_{h_{\ell+1}, h_{\ell}} \log(|z_{h_{\ell}}| + \epsilon) \right) W_{h_{\ell+1}, h_{\ell}} \frac{\mathrm{abs}'(z_{h_{\ell}})}{|z_{h_{\ell}}| + \epsilon} = m_{h_{\ell+1}} W_{h_{\ell+1}, h_{\ell}} \frac{\mathrm{abs}'(z_{h_{\ell}})}{|z_{h_{\ell}}| + \epsilon} 
$$

### The gate

$$
\frac{\partial g_{h_{\ell+1}}}{\partial z_{h_\ell}} = \frac{\partial g_{h_{\ell+1}}}{\partial \hat{g}_{h_{\ell+1}}} \frac{\partial \hat{g}_{h_{\ell+1}}}{\partial z_{h_\ell}} = \sigma'(\hat{g}_{h_{\ell+1}}) G_{h_{\ell+1},h_{\ell}} = \sigma(\hat{g}_{h_{\ell+1}}) (1 - \sigma(\hat{g}_{h_{\ell+1}})) G_{h_{\ell+1},h_{\ell}}
$$
