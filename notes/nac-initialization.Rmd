---
title: "NAC Initialization"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stats)
library(ggplot2)
library(dplyr)
```

# Deriving Optimal Initialization for NAC

The NAC is defined as:

$$
\begin{aligned}
z_{h_\ell} &= \sum_{h_{\ell-1}=1}^{H_{\ell-1}} w_{h_{\ell-1},h_\ell} z_{h_{\ell-1}} \\
w_{h_{\ell-1},h_\ell} &= \tanh(\hat{w}_{h_{\ell-1},h_\ell}) \sigma(\hat{m}_{h_{\ell-1},h_\ell})
\end{aligned}
$$

Defining $w_{h_{\ell-1},h_\ell}$ as a function of $\hat{w}_{h_{\ell-1},h_\ell}$, $\hat{m}_{h_{\ell-1},h_\ell}$, does not fundamentally how backpropergation is done.

The derivatives with respect to the weights are:

$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \hat{w}_{h_{\ell-1},h_\ell}} &= \frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} \frac{w_{h_{\ell-1},h_\ell}}{\partial \hat{w}_{h_{\ell-1},h_\ell}} \\
&= \frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} (1 - \tanh^2(\hat{w}_{h_{\ell-1},h_\ell})) \sigma(\hat{m}_{h_{\ell-1},h_\ell}) \\

\frac{\partial \mathcal{L}}{\partial \hat{m}_{h_{\ell-1},h_\ell}} &= \frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} \frac{w_{h_{\ell-1},h_\ell}}{\partial \hat{m}_{h_{\ell-1},h_\ell}} \\
&= \frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} \tanh(\hat{w}_{h_{\ell-1},h_\ell}) \sigma(\hat{m}_{h_{\ell-1},h_\ell}) (1 - \sigma(\hat{m}_{h_{\ell-1},h_\ell}))
\end{aligned}
$$

Thus, $\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}}$ is what needs to be expressed, similar to a typical MLP network.

$$
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial w_{h_{\ell-1},h_\ell}} = \delta_{h_\ell} z_{h_{\ell-1}}
$$

Likewise the backpropergation expression for $\delta_{h_\ell}$ remains the same:

$$
\delta_{h_\ell} = \sum_{h_{\ell+1}}^{H_{\ell+1}} \delta_{h_{\ell+1}} w_{h_\ell,h_{\ell+1}}
$$

As the expressions for $z_{h_\ell}$ and $\delta_{h_\ell}$ are the same as in Glorot et al., and the assumtion that the activation function $\theta(z_{h_\ell})$ has gradient $\theta'(z_{h_\ell}) = 1$ is fully true for the NAC unit. The same requirements that Glorot et al. gets should be furfilled in this case:

$$
\begin{aligned}
\mathrm{E}[W] &= 0 \\
\mathrm{Var}[W] &= \frac{2}{H_{\ell-1} + H_{\ell}}
\end{aligned}
$$

## Independent Sampling

What is drastically different is how $\mathrm{E}[W]$ and $\mathrm{Var}[W]$ are expressed. To simplify things, consider that $\hat{w}$ and $\hat{m}$ are independently uniformly distributed around 0.

$$
\begin{aligned}
W &\sim \tanh(\hat{W}) \sigma(\hat{M}) \\
\hat{W} &\sim ~ U[-r, r] \\
\hat{M} &\sim ~ U[-r, r] 
\end{aligned}
$$

The expectation requirement $\mathrm{E}[W] = 0$ is satistifed from this, since since $\tanh({\hat{W}})$ is an odd-function. Thus we have:

$$
\mathrm{E}[W] = \mathrm{E}[\tanh(\hat{W})]\mathrm{E}[\sigma(\hat{M})] = 0 \cdot \mathrm{E}[\sigma(\hat{M})] = 0
$$

The variance is more complicated, however as $\hat{W}$ and $\hat{M}$ are independent, it can be simplified to:
$$
\mathrm{Var}[W] = \mathrm{E}[\tanh(\hat{W})^2] \mathrm{E}[\sigma(\hat{M})^2] - \mathrm{E}[\tanh(\hat{W})]^2 \mathrm{E}[\sigma(\hat{M})]^2 = \mathrm{E}[\tanh(\hat{W})^2] \mathrm{E}[\sigma(\hat{M})^2]
$$

Thus the second moments can be analyzed independently. First for $\mathrm{E}[\tanh(\hat{W})^2]$:

$$
\begin{aligned}
\mathrm{E}[\tanh(\hat{W})^2] &= \int_{-\infty}^{\infty} \tanh(x)^2 f_{U[-r, r]}(x)\ \mathrm{d}x \\
&= \frac{1}{2r} \int_{-r}^{r} \tanh(x)^2\ \mathrm{d}x \\
&= \frac{1}{2r} \cdot 2 \cdot (r - \tanh(r)) \\
&= 1 - \frac{\tanh(r)}{r}
\end{aligned}
$$

Then for $\mathrm{E}[\tanh(\hat{M})^2]$:

$$
\begin{aligned}
\mathrm{E}[\sigma(\hat{M})^2] &= \int_{-\infty}^{\infty} \sigma(x)^2 f_{U[-r, r]}(x)\ \mathrm{d}x \\
&= \frac{1}{2r} \int_{-r}^{r} \sigma(x)^2\ \mathrm{d}x \\
&= \frac{1}{2r} \left(r - \tanh\left(\frac{r}{2}\right)\right)
\end{aligned}
$$

Finally this gives the variance:

$$
\mathrm{Var}[W] = \frac{1}{2r} \left(1 - \frac{\tanh(r)}{r}\right) \left(r - \tanh\left(\frac{r}{2}\right)\right)
$$

The requirement $\mathrm{Var}[W] = \frac{2}{H_{\ell-1} + H_{\ell}}$ can then be expressed as:

$$
\mathrm{Var}[W] = \frac{2}{H_{\ell-1} + H_{\ell}} = \frac{1}{2r} \left(1 - \frac{\tanh(r)}{r}\right) \left(r - \tanh\left(\frac{r}{2}\right)\right)
$$

Solving this is a simple matter of finding the root in:

$$
\frac{1}{2r} \left(1 - \frac{\tanh(r)}{r}\right) \left(r - \tanh\left(\frac{r}{2}\right)\right) - \frac{2}{H_{\ell-1} + H_{\ell}} = 0
$$

This root can be found using simple numeric optimization, as this function is well behaving.

```{r, echo=FALSE}
sigmoid = function (w) {
  return (1 / (1 + exp(-w)))
}

NAC.W.var = function (r) {
  return ((1 - tanh(r) / r) * (r - tanh(r / 2)) * (1 / (2*r))) 
}
```

```{r}
p = ggplot(data.frame(r = c(0.001, 50)), aes(x = r)) +
      stat_function(fun = NAC.W.var) +
      ylim(c(0, 0.5)) +
      labs(y = 'Var[W]')
print(p)
```


Note that since $Var[W] < \frac{1}{2} \forall r \in [0, \infty]$, it is only possible to find an optimal $r$ for $H_{\ell-1} + H_\ell > 4$.

### Numerical analysis

As an aid for finding the numerical optimum it can be seen via numerical analysis that the solution for $r$ is always $r \in [0, 10]$.

```{r, echo=FALSE}
NAC.W.r = function (fan_sum) {
  return (uniroot(function (r) NAC.W.var(r) - (2 / fan_sum), interval=c(0, 10), f.lower=-1)$root)
}
```

```{r}
fan.values = c(seq(5, 2**7, 1), seq(2**7, 2**13, 8))
r.by.fan = data.frame(list(
  fan = fan.values,
  r = sapply(fan.values, NAC.W.r)
))

p = ggplot(r.by.fan, aes(x = fan, y=r)) +
    geom_line() +
    scale_x_continuous(trans='log2') +
    ylim(c(0, 10)) +
    labs(x='fan')
print(p)
```

Numerically estimating the variance for $H_{\ell-1} + H_{\ell} = 20$ over 1000000 samples, shows that symbolic analysis appears to be correct:

```{r, echo=FALSE}
uniform.sampler = function (r, num.samples) {
  w.hat = runif(num.samples, -r, r)
  m.hat = runif(num.samples, -r, r)
  return (tanh(w.hat) * sigmoid(m.hat))
}

nac.sampler = function (fan, num.samples) {
  return (uniform.sampler(NAC.W.r(fan), num.samples))
}

he.sampler = function (fan, num.samples) {
  return (uniform.sampler(sqrt(6 / fan), num.samples))
}

glorot.sampler = function (fan, num.samples) {
  return (uniform.sampler(sqrt(3 / fan), num.samples))
}

fan = 10
num.samples = 1000000

samples = rbind(
  data.frame(list(
    samples = he.sampler(fan*2, num.samples),
    rule = 'he'
  )),
  data.frame(list(
    samples = nac.sampler(fan*2, num.samples),
    rule = 'nac'
  )),
  data.frame(list(
    samples = glorot.sampler(fan*2, num.samples),
    rule = 'glorot'
  ))
);
```

```{r}
samples %>%
  group_by(rule) %>%
  summarize(mean=fan * mean(samples), variance=fan * var(samples))
```

An issue that has not been analyized so far, is the distribution of $W$ itself. Considering different initialization proceadures, the histrogram is numerically estimated over 1000000 samples for $H_{\ell-1} + H_{\ell} = 20$. 

```{r}
p = ggplot(data=samples, aes(x=samples, fill=rule)) +
  geom_histogram(aes(y=..density..), breaks=seq(-1, 1, by=0.01), alpha=0.7) +
  labs(x = 'W')
print(p)
```

This shows a vital flaw in NAC initlaization presented here. Namely, that sampling $\hat{W}$ and $\hat{M}$ unformly independent doesn't not guarantee that $W$ has a reasonable distribution.
