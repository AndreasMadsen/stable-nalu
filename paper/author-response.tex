\documentclass{article}

\usepackage{neurips_2019_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{lipsum}

\begin{document}
%
% Put more emphasis on our ability to handle small and negative numbers.
\textbf{Initialization}
%#1If an initialization scheme results in $E[z_{h_l}]\ne 0$, does that mean the optimization would be necessarily difficult?\\

%#1Show the intuition behind $E[z_{h_l}]$ needs to be zero rather than simply citing Glorot.\\
%\subsection{answer}
% two liner omkring dette
\textbf{Fair comparison}
%#1It's not surprising to me that the proposed multiplication unit NMU outperforms $NAC_\cdot$, since $NAC_\cdot$ is targeting a more ambitious division operation as well, so the comparison is unfair.\\

%#3Comparisons to the original model under the set of conditions of the original paper, and slowly showing/developing the issues of that model, i.e. starting with a fair comparison and then showing where the original model fails.\\

%#3Balance the conclusion, as it currently states the advantaged of NAU and NMU without mentioning their drawbacks, which seem significant wrt NALU.\\
%\subsection{answer}
To emcompass a more fair comparison we have extended the models we test to include a $NAC_{\bullet,\sigma}$ that is parametized by a sigmoid constrained weight matrix $W = \sigma(\hat{W})$ such that it is bounded between $[0,1]$ and thus only supports multiplication similar to the NMU.

Maybe include $NAC_{\bullet,\Sigma}$, 0-1, linear, regularization (sparsity, mnist regularization)\\

\textbf{Better experiments}\\
%#1While this work gets better performance compared to the baseline, the novelty is only limited to the theoretical analyses of NALU and a new parameterization, which might not be enough contributions to be accepted by NeurIPS.\\

%#2better experimental settings\\

%#2better ablation study\\

%\subsection{answer}
We have extended the experimental section with Mnist and more synthetic tasks.

In our synthetic extensions we test redundancy in the multiplication layer. Under current setup the model goes from 100 to (addition) 2 to (multiplication) 1. As our theoretical results shows that the $NAC_{\bullet}$ has exploding variance for increased hidden sizes we test the model with hidden sizes ranging from 2 to 10. Our results confirm this issue as the $NAC_{\bullet}$ cannot converge for larger hidden sizes in the multiplication layer whereas the NMU can. Please see figure X
We believe a large hidden size is important. This type of model could be useful for systems with unknown dynamics (e.g. differential equations) where we cannot make assumptions about the size of the equation, thus we need to have large hidden states that might contain redundancy.

We copied the sequential MNIST task from the NALU paper, but testing sequential multiplication instead of addition.
Our measures are the same as for the simple function task; ability to converge to within a defined test error, sparsity and time to solve for converged models.
We train on a sequence of two numbers and measure performance on sequences of two numbers and nine numbers (we didnt go higher due to computational constraints).
Our results for training the NMU, NALU, $NAC_{\bullet}$, $NAC_{\bullet,\sigma}$ with varying hidden sizes for the multiplication layer shows that the NMU converges more consistently and more than x10 faster on a real deep learning problem.

%Run it at 15, 30

%\section{Regularization}
%#1Wouldn't weight clipping suffer saturation issues as well? If it was the regularizations that helped the most, probably adding them to NALU would work as well.\\
%#3Would the same biasing auxiliary loss presented in the paper help the original NALU model too?\\
%#3L175 why did you chose $\lambda_bias$ in this way?\\
%\subsection{answer}
\textit{Wouldn't weight clipping have saturation issues?} The linear unit should have a gradient of 1 at all positions.\\
\textit{Regularization in the NALU?} It might be argued that the regularization gives the NMU and unfair advantage.
To that extend we have trained a variant elaborated in previous response that includes all the bells and whistles of our regularization.\\
\textit{L175 why did you chose $\lambda_bias$ in this way?} We did find that learning the problem and sparsifying can be considered two different optimization objectives.
We now use a spilened regularization that is 0 for the first million iterations, gradually scales from 1 to 2 million and then flattens.
We find that this help with convergence as well.\\

\textbf{Original paper}\\
%#3comparison of performance with the original models on a set of arithmetic tasks (significance: medium to low)\\

%#3the resulting experiments are specific to pick out the disadvantages of the original model\\

%#3why does the multiplication perform satisfactory in the original paper, but fails in this paper\\

%#3how come NALU performs great in the original paper, but is often worse than the linear model in this paper? The difference wrt the results in the original paper needs to be properly explained in the paper.\\

%#3 the paper does a superb analysis of the issues, but it is unclear whether these issues make the original model unusable (from the original paper it seems that is not the case) and how difficult do they make it train (what is the standard number of iterations NALU is trained for)?\\
%\subsection{answer}
\textit{Why does the NALU work so poorly?} It's difficult to assess exactly why it does not work as Trask has not published his code.
We have spent a significant amount of time and contacted Trask many with multiple emails where we received vague responses.
The tasks that we pose are the closest we could possibly get to Trasks original setups (which are also vaguely defined).
Our conclusion is that Trask has picked some tasks which where if you run enough seeds your model will eventually be close enough to a solution to find it.
It is to be mentioned that he use the same weight matrix for addition and multiplication (which intuitively does not make sense), but might help with optimization on specific tasks.
Our conclusion is supported by social media (github,reddit, twitter) where noone has been able to reproduce the results.
Furthermore, not a single paper citing Trask is successfully using his method or extending it (as we do).
\textit{Are the experiments to pick a disadvantage on the NALU?} Our goal is the opposite, we have taken the exact same task that Trask (simple function task and mnist) and simply modified all task hyperparameters (hidden size, ratio, subsets, input size) to test the sturdiness of the models.\\

\textbf{Other}\\
%#3Is the parameter sparsity really a necessity? Concretely, in L166-167 can you verify the correctness of this assumption?\\

%#3How often are the parameters close to {-1, 0, 1} in NALU, when it learns the function well?\\

%#3are the advantages of the NAU and NMU bring stronger than the additional set of assumptions, such as the need to break NALU into two models without using the gating mechanism, and the inability to do division\\

%#3did NAC+ really need fixing (other than improving the training)\\

%#3Also, the presented models specialize for three arithmetic operations where the original model was tested on three more operations.\\

%#3Are these models applicable to those operations too?\\
%\subsection{answer}
\textit{Is the parameter sparsity a necessity?} For multiplication it is a requirement.
Furthremore, a point that we do not emphasize much in this paper is interpretability, which binary values help with.%include proof
\textit{How often are the parameters close to {-1, 0, 1} in the NALU} In the result section we provide a sparsity measure of the models that did converge, which measures %include formula for sparsity measure
\textit{Is the NMU able to do division, squaring and the square root?} The NMU is able to do squaring $x**2$ as it does multiplication (same way the NALU does it), it is not able to do division, however it is our firm believe that neither is the NALU (which is also pointed in their original paper, see table 1).
It is true that the NMU, by design, cannot perform square roots, which the NALU is theoretically able to learn.
On the other hand, the NMU can do negative numbers and small numbers (below $\epsilon$).
\textit{Are the assumptions of splitting the NALU worth the effort?} We only include the NALU for legacy purposes and to emphasize that it does not work.
Our true comparison is with the subunits $NAC_{\bullet}$, $NAC_{+}$ as well as other subunit variants such as $NAC_{\bullet, \sigma}$ and $NAC_{\bullet, \Sigma}$ to highlight the optimization issues with using the log-exp method and using tanh-sigmoid weight constraints.
If we cannot make the subunits work, there is little hope for the NALU when we combine them.
\textit{Does the $NAC_{+}$ need fixing?} Using a simpler weight matrix makes optimization easier, reduce parameters and allows discrete weights. However, it is definitely an minor novelty.
%#3 the equation in (1) does not cover the example of the learning function in L60, since the ordering of the elements of the vectors is static, and in the NALU paper, (as you did later) they sum up the first N elements, and apply the operation on the summation of the rest of the vector elements. The equation (1) seems like it's written to encompass a large space of solutions, but is a matter of a fact quite limited. The issue of operator precedence easily pops up there too.\\
%
\end{document}