\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% COMMENT for anonymous submission
%\def\nonanonymous{}

\ifdefined\nonanonymous
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2019}
% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2019}
\else
% ready for submission
\usepackage{neurips_2019}
\fi

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Not part of the offical NeurIPS template
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{layouts}
\usepackage{float}
\usepackage{todonotes}
\usepackage{enumitem}

% Algorithms
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algnewcommand{\Let}[2]{\State #1 $\gets$ #2}
\algrenewcommand\Call[2]{\textproc{#1}(#2)}

% Kabel Tables
\usepackage{multirow}
\usepackage{tabu}
\captionsetup[table]{skip=5pt}

\let\cite\citep
\title{Neural Arithmetic Units}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Andreas Madsen$^{\dag\ddag}$ \\
  \texttt{amwebdk@gmail.com}
  \AND
  Alexander Rosenberg Johansen$^{\dag}$ \\
  \texttt{aler@dtu.dk} \\
  \\
$^\dag$Technical University of Denmark \quad
$^\ddag$Computationally Demanding
}

\begin{document}

\maketitle

\begin{abstract}
%What’s the domain?
Exact addition, subtraction, multiplication, and division of real numbers present a unique learning challenge for machine learning models.
%What’s the issue?
Neural networks can approximate complex functions by learning from labeled data.
However, when extrapolating to out-of-distribution samples on arithmetic operations neural networks often fail. Learning the underlying logic, as opposed to an approximation, is crucial for applications such as comparing, counting, and inferring physical models.
%What’s your contribution?
Our proposed Neural Addition Unit (NAU) and Neural Multiplication Unit (NMU) can learn the underlying rules of real number addition, subtraction, and multiplication by merely observing arithmetic examples and using backpropagation.
%rely on constrained weights to learn rules using  and extrapolate well beyond the training distribution on addition, subtraction and multiplication.
%We propose a plug-and-play differentiable neural unit that can be trained using stochastic gradient descent to learn addition, subtraction and multiplication between units of a hidden layer.
%Why is it novel?
%What’s interesting about it?
The core novelty of NAU and NMU is their ability to efficiently learn sparse weights, which allows the unit to perform exact arithmetic operations.
%The NAU can perform addition and subtraction using a linear layer of constrained weights.
%The NMU can perform multiplication using an accumulative product of the input using gating with an identity function to mask out unwanted elements.
%The weights are optimized with stochastic gradient decent with regularization for sparsity.
%How does it perform?
The NAU and NMU are inspired by the underlying arithmetic components of the Neural Arithmetic Logic Unit (NALU).
Through analytic and empirical analysis we justify how the NAU and NMU improve over the Neural Arithmetic Logic Unit (NALU), a linear regression model and a ReLU based multi-layer perceptron (MLP).
Our NAU and NMU have fewer parameters, converges more consistently, learn faster, and have more meaningful discrete values than the NALU and its arithmetic components.\ifdefined\nonanonymous\footnote{Implementation is available on GitHub: \url{https://github.com/AndreasMadsen/stable-nalu}.}\fi
\end{abstract}

\input{sections/introduction}
\input{sections/methods}
\input{sections/results}
\input{sections/related-work}
\input{sections/conclusion}

\clearpage
\ifdefined\nonanonymous
\subsubsection*{Acknowledgments}
We would like to thank Andrew Trask and the other authors of the NALU paper, for highlighting the importance and challenges of extrapolation in Neural Networks. We would also like to thank the students Raja Shan Zaker Kreen and William Frisch Møller from The Technical University of Denmark, who initially showed us that the NALU does not converge consistently.
\fi

\bibliographystyle{plainnat}
\bibliography{bibliography}

\newpage
\appendix
\input{appendix/gradient-derivatives}
\clearpage
\input{appendix/moments}
\clearpage
\input{appendix/simple-function-task}
\clearpage

\end{document}