\section{Simple function task}
\subsection{Dataset generation}
\label{sec:appendix:simple-function-task:data-generation}

\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{graphics/function_task_static_problem.pdf}
\caption{Dataset is parameterized into ``Input Size'', ``Subset Ratio'', ``Overlap Ratio'', an Operation (here showing multiplication), ``Interpolation Range'' and ``Extrapolation Range'' from which the data set sampled.}
\label{fig:simple-function-task-problem}
\end{figure}

All datasets in the simple function task experiments are generated using algorithm \ref{alg:dataset-sampling}. Its parameters are visualized in figure \ref{fig:simple-function-task-problem}.

\begin{algorithm}[h]
  \caption{Dataset sampling algorithm}
  \begin{algorithmic}[1]
    \Function{Dataset}{${\Call{Op}{\cdot, \cdot}: \mathrm{Operation}}$, ${i: \mathrm{Input Size}}$, ${s: \mathrm{Subset Ratio}}$, ${o: \mathrm{Overlap Ratio}}$, ${\hspace{3cm}R: \mathrm{Range}}$}
      \Let{$\mathbf{x}$}{\Call{Uniform}{$R_{lower}, R_{upper}, i$}} \Comment{Sample $i$ elements uniformly}
      \Let{$k$}{\Call{Uniform}{$0, 1 - 2s - o$}} \Comment{Sample offset}
      \Let{$a$}{\Call{Sum}{$\mathbf{x}[ik:i(k+s)]$}} \Comment{Create sum $a$ from subset}
      \Let{$b$}{\Call{Sum}{$\mathbf{x}[i(k+s-o):i (k+2s-0)]$}} \Comment{Create sum $b$ from subset}
      \Let{$t$}{\Call{Op}{$a, b$}} \Comment{Perform operation on $a$ and $b$}
      \State \Return{$x, t$}
    \EndFunction
  \end{algorithmic}
  \label{alg:dataset-sampling}
\end{algorithm}

\subsection{Arithmetic operations comparison - all models}

\begin{table}[h]
\caption{Model definitions}
\label{tab:model-defintions}
\centering
\begin{tabular}{r l l}
\toprule
 Model & Layer 1 & Layer 2 \\
 \midrule
 NMU & NAU & NMU \\
 NAU & NAU & NAU \\
 $\mathrm{NAC}_{\bullet}$ & $\mathrm{NAC}_{+}$ & $\mathrm{NAC}_{\bullet}$ \\
 $\mathrm{NAC}_{+}$ & $\mathrm{NAC}_{+}$ & $\mathrm{NAC}_{+}$ \\
 NALU & NALU & NALU \\
 Linear & Linear & Linear \\
 ReLU & ReLU & ReLU \\
 ReLU6 & ReLU6 & ReLU6 \\
 \bottomrule
\end{tabular}
\end{table}

Results for all models on addition, subtraction, and multiplication can be found in table \ref{tab:function-task-static-defaults-all}.

\label{sec:appendix:function-task-static-all-comparision}
\input{results/function_task_static_all.tex}

\subsection{Ablation study}
To validate our model, we perform an ablation on the multiplication problem.

Our ablation study (table \ref{tab:function-task-static-ablation}) show that regularization have little effect in terms of success rate. As it is analytically known that there is no gradient outside of $w \in [0,1]$ for the NMU, the conclusion must be that the optimal weight initialization for the default dataset parameters and tested seeds, does not cause any weights to accidentally break out of $w \in [0,1]$. The sparse regularizer for multiplication have no sparsity effect, as only a sparse solution is a valid solution for multiplication. Although as seen in appendix \ref{sec:appendix:simple-function-task:regualization}, sparsity regularization can improve convergence.
\input{results/function_task_static_ablation.tex}

Not allowing a multiplicative identity ($\mathbf{z} = \mathbf{W} \odot \mathbf{x}$), works when there is only two hidden units in the multiplication layer, as no multiplicative identity is necessary. However, for larger a hidden size as seen in figure \ref{fig:simple-function-static-ablation-hidden-size} identity becomes necessary.
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_ablation_hidden_size.pdf}
\caption{Compares NMU with NMU without identity, with different input size (hidden layer unit size) to the multiplication layer.}
\label{fig:simple-function-static-ablation-hidden-size}
\end{figure}

\subsection{Regularization}
\label{sec:appendix:simple-function-task:regualization}
A high sparsity regularization constant can help the model to converge faster. However, a regularization constant too high have have the inverse effect as well, or even make it impossible for the model to converge.
regularizer
In these experiments, the constant $c$ in equation \ref{eq:regualizer-experiment} is varied. See results in figure \ref{fig:simple-fnction-static-regularizer-add}, \ref{fig:simple-fnction-static-regularizer-sub}, and \ref{fig:simple-fnction-static-regularizer-mul}.
\begin{equation}
\lambda_{\mathrm{bias}} = c \cdot (1 - \exp(-10^5 \cdot t))
\label{eq:regualizer-experiment}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_regualization_add.pdf}
\caption{Shows the effect of the regularizer $\lambda_{\mathrm{bias}}$, on the simple function task problem for the $\bm{+}$ operation.}
\label{fig:simple-fnction-static-regularizer-add}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_regualization_sub.pdf}
\caption{Shows the effect of the regularizer $\lambda_{\mathrm{bias}}$, on the simple function task problem for the $\bm{-}$ operation.}
\label{fig:simple-fnction-static-regularizer-sub}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_regualization_mul.pdf}
\caption{Shows the effect of the regularizer $\lambda_{\mathrm{bias}}$, on the simple function task problem for the $\bm{\times}$ operation.}
\label{fig:simple-fnction-static-regularizer-mul}
\end{figure}

