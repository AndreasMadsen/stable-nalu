\section{Conclusion}

An recent approach to learn arithmetic operations from data using stochastic gradient decent, has analytical and empirical concerns.
We have shown analytical how the NAU and NMU can be initialized optimally. In experiments stress-testing arithmetic operations, the NAU and NMU consistently outperforms recent approaches and neural networks.
While the NMU can not divide, it is capable of extrapolate into the negative range for multiplication.

% Future work:
% Depth, applications (convergence is still slow), gating, division, test higher precision.

