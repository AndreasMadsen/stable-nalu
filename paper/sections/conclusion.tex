\section{Conclusion}
We have investigated how a function based on constraint weights can extrapolate well on arithmetic operations and if it can learn the weights by stochastic gradient descent.
Inspired by the recently proposed Neural Arithmetic Logic Unit (NALU), we propose the Neural Addition Unit (NAU) and Neural Multiplication Unit (NMU), which has more consistent convergence on a range of addition, subtraction, and multiplication tasks than the NALU.4
Beyond convergence capabilities, our modifications makes the NMU capable of extrapolating to numerically small numbers and the negative range, which has previously been impossible.
Our analytic analysis highlights that learning division is challenging as division close to zero creates explosions. \todo{rewrite}

% Future work:
% Depth, applications (convergence is still slow), gating, division, test higher precision.

