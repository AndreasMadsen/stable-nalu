\section{Introduction}

The ability for neurons to hold numbers and do arithmetic operations has been documented in both humans, non-human primates \cite{nieder-neuronal-number}, newborn chicks \cite{rugani-arithmetic-chicks} and bees \cite{gallistel-numbers-in-brain}.
In our race to solve intelligence we have put much faith in neural networks, which in turn has provided unparalleled and often superhuman performance in many tasks requiring high cognitive ability \cite{natureGo,googleNMT,resnet}.
However, when using neural networks to solve simple arithmetic problems, such as counting, they systematically fail to extrapolate \cite{stillNotSystematic,suzgun2019evaluating,trask-nalu}.

In this paper, we analyze and improve parts of the recently proposed Neural Arithmetic Logic Unit (NALU) \cite{trask-nalu}.
The NALU is a neural network layer with two modules.
The two modules, $\text{NAC}_{+}$ for addition/subtraction and $\text{NAC}_{\bullet}$ for multiplication/division, are softly gated between using the sigmoid function.
By using trainable weights, and restricting the weights towards $\{-1,0,1\}$ the $\text{NAC}_{+}$ is able to approximate addition and subtraction of hidden units in the previous layer.
The $\text{NAC}_{\bullet}$ extends this capability with to achieve multiplication and division. \todo{The text before is repeating a lot. Consider just deleting it.}
These rules should be learned only by observing arithmetic input-output pairs and using backpropagation\cite{rumelhart1986learning}.

In practice, training the NALU can be cumbersome.
Through an gradient analysis of main components in the NALU, the weight matrix constraint, the multiplication construction, and the gating\todo{We don't discuss gating} we present the following findings:

\begin{itemize}
\item The weight matrix constraint in the NALU, under zero expectation of the mean layer value\cite{glorot-initialization}, has a gradient of zero.

\item The $\text{NAC}_{\bullet}$ have a treacherous optimization space with unwanted global minimas (as shown in figure \ref{fig:nac-mul-eps-issue}), exploding/vanishing gradients.

\item When performing using the addition module $\text{NAC}_{+}$, we observe that the wanted weight matrix values of ${-1, 0, 1}$ is rarely found.

\item The gating between the addition/subtraction component and the multiplication/division component has a gradient that, in expectation, bias' towards the $\text{NAC}_{\text{+}}$ \todo{Properly we shouldn't discuss gating in details. And that analysis is extreamly complex.}.
\end{itemize}

Motivated by these convergence and sparsity issue, we propose alternative formulations of the $\text{NAC}_{+}$ and $\text{NAC}_{\bullet}$, which we call the Neural Addition Unit (NAU) and Neural Multiplication Unit (NMU).
That is, we will assume that the appropriate operation is already known, or can empirically be found by varying the network architecture (oracle gating). \todo{We some wage mention of why we don't consider the gating mechanism.}

We propose an alternative formulation of the matrix constraint that uses a clipped linear activation, a regularizer that bias throwards sparse solutions, and fundamentally reformulate the multiplication unit to be partially linear. All of which significantly improves upon the existing $\text{NAC}_{+}$ and $\text{NAC}_{\bullet}$ units as shown through extensive testing on synthetic tasks and images.

%It is fair to assume that the ability to perform arithmetic operations is crucial modern intelligence.
%However, Neural networks(nature, yann lecun), even though it has great expressiveness, has shown an inability to count severe lacks the ability to explicitly only has the ability to has an extreme capacity to interpolate complex functions from simply observing data (ICLR best paper 2017).
%However, even when extrapolating to unseen data we often find a sharp in performance.\cite{missing-cite:image-with-modification}(systematic, nalu, Anders paper).
%Such inability to extrapolate either requires the training distribution to cover the entire distribution (DAGGER) or simply restrics certain systems to be built all together.

%Our solution

%Experiments

%Results





%Remove
%So far, there have not been much research in training exact arithmetic operations for extrapolation. The most important research, is by far, the recently proposed NAC and NALU \cite{trask-nalu}. However, as we will show both analytically and empirically, their results are highly exaggerated, as the proposed arithmetic units are extremely difficult to make convergence consistently. In fact, even without gating mechanism their multiplication operator can't converge on extremely simple problems. For the cases of addition there do converge, a nearly sparse weight matrix of values close to ${-1, 0, 1}$ is rarely found, which also contradicts one of their core claims.
%Motivated by these convergence and sparsity issue, we focus on improving the arithmetic units themself, without considering the gating mechanism in NALU. That is, we will assume that the appropriate operation is already known, or can empirically be found by varying the network architecture which is very common when developing neural networks.