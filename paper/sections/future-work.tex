\section{Related work}
Pure neural models that learns arithmetic tasks through backpropagation has previously been attempted.
Either by utilizing relying on convolutions, gating, differentiable memory and/or attention architectures\cite{NeuralGPU,GridLSTM,NTM}.
Some of the results are close to perfect performance, however, they are constrained to only work on the integer set of numbers and requires well defined arithmetic setups such as binary representations of numbers for input and output.
We do not try and test the expressiveness of current approximate approaches, but instead develop a fundamental new unit of computation with weight constraints to learn exact decimal operations without any assumptions of binary representations.

To handle decimal operations the Neural Arithmetic Expression Calculator\cite{NAEC} propose learning the individual components needed for decimal operations.
They repeatedly combine the in a program induction way with a memory-encoder-decoder architecture trained with reinforcement learning.
While this model has the ability to dynamically handle different types of expressions than our solution, they do not generalize much beyond interpolation length.

We use the NMU to do a subset-selection, which is then followed by a summation, in order to combine information from the input vector.
An alternative, fully differentiable version, is to use a gumbel-softmax to perform exact subset-selection\cite{DSS}.
This, however, has the restriction of having to use a predefined size of the subset, which is a strong assumption that our units does not have.

% NALU gating issues, use a "model selection" theory (Alexander stuff).

% 1. Differentiable Subset Sampling  https://arxiv.org/abs/1901.10517v1
% 
