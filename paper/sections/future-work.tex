\section{Related work}
Developing pure neural models to learn arithmetic tasks has been attempted by using deep learning architectures relying on convolutions, gating, differentiable memory and attention architectures\cite{NeuralGPU,GridLSTM,NTM}.
Some of the results are close to perfect performance, however, with constraints of only working on the integer set of numbers and requiring well defined arithmetic setups, such as binary representations of numbers for input and output.
We do not try and test the expressiveness of current approximate approaches, but instead develop a fundamental new unit of computation with weight constraints to learn the exact decimal operations without any assumptions of binary representations.

To handle decimal operations the Neural Arithmetic Expression Calculator\cite{NAEC} propose learning the individual components needed for decimal operations.
They repeatedly combine the in a program induction way with a memory-encoder-decoder architecture trained with reinforcement learning.
While this model has the ability to dynamically handle different types of expressions than our solution, they do not generalize much beyond interpolation length.

We use the NMU to do a subset-selection, which is then followed by a summation, in order to combine information from the input vector.
An alternative, fully differentiable version, is to use a gumbel-softmax to perform exact subset-selection\cite{DSS}.
This, however, has the restriction of having to use a predefined size of the subset, which is a strong assumption that our units does not have.

% NALU gating issues, use a "model selection" theory (Alexander stuff).

% 1. Differentiable Subset Sampling  https://arxiv.org/abs/1901.10517v1
% 
