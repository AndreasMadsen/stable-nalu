\section{Experimental results}

\subsection{Simple function task}

Our simple function task samples an input vector $\mathbf{x}$ from a uniform distribution. From this input vector, the sum of two subsets $a$ and $b$ are then computed. Finally the target $t$ is then an operation performed on $a$ and $b$ (e.g. $a \cdot b$). This is identical to the task by the same name in the Original NALU paper \cite{trask-nalu}. Except that we parameterize it in order to compare the models for different configurations, see figure \ref{fig:simple-function-task-problem}. To make comparison simple, we define a set of default parameters (table \ref{tab:simple-function-task-defaults}) and only vary one of them at the time.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{graphics/function_task_static_problem.pdf}
\caption{Dataset is parameterized into ``Input Size'', ``Subset Ratio'', ``Overlap Ratio'', an Operation (here showing multiplication), ``Interpolation Range'' and ``Extrapolation Range'' from which the data set sampled.}
\label{fig:simple-function-task-problem}
\end{figure}

\begin{table}[H]
\caption{Default dataset parameters}
\label{tab:simple-function-task-defaults}
\centering
\begin{tabular}{r l}
\toprule
 Parameter Name & Default Value \\
 \midrule
 Input Size & 100 \\
 Subset Ratio & 0.25 \\
 Overlap Ratio & 0.5 \\
 Interpolation Range & $U[1,2]$ \\
 Extrapolation Range & $U[1,6]$ \\
 \bottomrule
\end{tabular}
\end{table}

Normally one would report the interpolation and extrapolation loss. However, the complex approximations that one would typically see in neural networks are not considered good enough. The goal is to achieve a solution that is sufficiently close to a perfect solution. Because there can be many valid permutations of a perfect solution, especially for addition, a solution is judged firsts on the final extrapolation error, and then on a sparsity error.

All errors; extrapolation, interpolation, and sparsity are computed every 1000 iterations for 2048 new observations. Because the interpolation and extrapolation errors are quite noisy, even for a near perfect solution. The median over the last 100 measurements is reported.

A model is considered a success if the extrapolation median is less than $\epsilon = 0.2$. This value was acquired by inspecting the error of a near perfect solution. \todo{Do better.}

The sparsity error is computed as in equation \ref{eq:sparsity-error}, and is only considered for the models that did solve the last.
\begin{equation}
E_\mathrm{sparsity} = \max_{h_{\ell-1}, h_{\ell}} \min(|W_{h_{\ell-1},h_\ell}|, |1 - |W_{h_{\ell-1},h_\ell}||)
\label{eq:sparsity-error}
\end{equation}

The first iteration for which $\mathrm{extrapolation} < \epsilon$, is also reported. Again, only models that did solve the task are considered.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{paper/results/function-task-static-example.pdf}
\caption{Example of exploration error, interpolation error, and sparsity error, for the task $a \cdot b$ with the default dataset parameters.}
\end{figure}

\subsubsection{Very simple function}

To empirically validate the theoretical problems with $\mathrm{NAC}_{\bullet}$, let's consider the very simple problem shown earlier in figure \ref{fig:nac-mul-eps-issue}. That is $x \in \mathbb{R}^4$, $a = x_1 + x_2$ and $b = x_1 + x_2 + x_3 + x_4$. The solution to this problem is that seen in equation \ref{eq:very-simple-function-ideal-solution}.
\begin{equation}
    \mathbf{W}_1 = \begin{bmatrix}
    1 & 1 & 0 & 0 \\
    1 & 1 & 1 & 1
    \end{bmatrix}, \mathbf{W}_2 = \begin{bmatrix}
    1 & 1
    \end{bmatrix}
    \label{eq:very-simple-function-ideal-solution}
\end{equation}

Each model is trained 100 times with different seeds, and stopped after 200000 iterations. Default Adam optimization is used, with a mini-batch size of 128 observations. The results (as seen in table \ref{tab:very-simple-function-results}), shows that NMU have a much higher success rate and converges much faster. The few cases that did not converge successfully are because of underflow when $w = 0$ in the NMU layer.

\input{results/simple_mul.tex}

\subsubsection{Defaults}

To compare on the exact same task as used in the Original NALU paper \cite{trask-nalu}. We report the success rate, the iteration which the model converged, and the sparsity error in table \ref{tab:function-task-static-defaults}. The models are trainined for 5000000 iterations. Default Adam optimization is used, with a mini-batch size of 128 observations.

As seen the NMU model, unlike $\mathrm{NAC}_{\bullet}$ always converges, and even when $\mathrm{NAC}_{\bullet}$ converges the NMU models converges about twice as fast.

The NAU model, like the $\mathrm{NAC}_{+}$ model, always converges. However, NAU model converges more than twice as fast. It even converges faster than a Linear model. Also notice that the $\mathrm{NAC}_{+}$ model have a poor sparsity error. This is because it doesn't bias to $\{-1, 0, -1\}$.

\input{results/function_task_static.tex}

\subsubsection{Exploration of dataset parameters}

Finally, the parameters from which the dataset is constructed are considered for just the multiplication problem ($a \cdot b$). The setup is the same the results from table \ref{tab:function-task-static-defaults}. The results are visualized in in figure \ref{fig:simple-fnction-static-input-size}, \ref{fig:simple-fnction-static-overlap}, and \ref{fig:simple-fnction-static-subset}.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_input_size.pdf}
\caption{Shows the effect of the input size. Errors bars shows the upper and lower 10\% quantile, computed over 10 different seeds for each configuration. The center shows the mean of those 10 observations.}
\label{fig:simple-fnction-static-input-size}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_overlap.pdf}
\caption{Shows the effect of the overlap ratio. Errors bars shows the upper and lower 10\% quantile, computed over 10 different seeds for each configuration. The center shows the mean of those 10 observations.}
\label{fig:simple-fnction-static-overlap}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{results/simple_function_static_subset.pdf}
\caption{Shows the effect of the subset ratio. Errors bars shows the upper and lower 10\% quantile, computed over 10 different seeds for each configuration. The center shows the mean of those 10 observations.}
\label{fig:simple-fnction-static-subset}
\end{figure}

\subsection{sequential MNIST}

\todo[inline]{Still waiting for results}

\begin{figure}[H]
\centering
\includegraphics[scale=1]{graphics/mnist_sequence_problem.pdf}
\caption{Lorem Ipsum.}
\end{figure}

