{"cells":[{"source":"# Change directory to VSCode workspace root so that relative path loads work correctly. Turn this addition off with the DataScience.changeDirOnImportExport setting\nimport os\ntry:\n\tos.chdir(os.path.join(os.getcwd(), '..'))\n\tprint(os.getcwd())\nexcept:\n\tpass\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["import math\n","import torch\n","import numpy as np\n","\n","torch.manual_seed(0)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Data Generation"],"metadata":{}},{"source":["class Dataset:\n","    def __init__(self, vector_size=6, seed=None):\n","        self.rng = np.random.RandomState(seed)\n","        self.vector_size = vector_size\n","        self.a_start = 0\n","        self.a_end = 2\n","        self.b_start = 4\n","        self.b_end = 6\n","\n","    def batch(self, batch_size=128):\n","        v = self.rng.uniform(0, 1, size=(batch_size, self.vector_size))\n","        a = np.sum(v[:, self.a_start:self.a_end], axis=1)\n","        b = np.sum(v[:, self.b_start:self.b_end], axis=1)\n","        t = a + b\n","\n","        return (torch.tensor(v, dtype=torch.float32), torch.tensor(t[:, np.newaxis], dtype=torch.float32))\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Model\n",""],"metadata":{}},{"source":["class RegualizedLinearLayer(torch.nn.Module):\n","    \"\"\"Implements the Gumbel NAC (Neural Accumulator)\n","\n","    Arguments:\n","        in_features: number of ingoing features\n","        out_features: number of outgoing features\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, name=None):\n","        super().__init__()\n","        self.name = name\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        self.tau = torch.nn.Parameter(torch.tensor(1), requires_grad=False)\n","\n","        self.W_hat = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.register_parameter('bias', None)\n","\n","    def reset_parameters(self):\n","        # Initialize to zero, the source of randomness can come from the Gumbel sampling.\n","        torch.nn.init.normal_(self.W_hat, 0, 0.1)\n","        torch.nn.init.constant_(self.tau, 1)\n","\n","    def set_tau(self, tau):\n","        self.tau.fill_(tau)\n","\n","    def set_iteration(self, iteration):\n","        self.iteration = iteration\n","\n","    def regualizer(self):\n","        reg = (1 - self.tau) * torch.sum(self.W_hat**2 * (1 - torch.abs(self.W_hat))**2)\n","        return reg\n","\n","    def forward(self, input):\n","        if self.iteration % 1000 == 0:\n","            print(f'{self.name}.W')\n","            print(self.W_hat)\n","\n","        return torch.nn.functional.linear(input, self.W_hat, self.bias)\n","\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}'.format(\n","            self.in_features, self.out_features\n","        )\n","\n","class Network(torch.nn.Module):\n","    def __init__(self, model_name, vector_size=6):\n","        super().__init__()\n","        self.model_name = model_name\n","\n","        if model_name == 'GumbelNAC':\n","            self.layer_1 = RegualizedLinearLayer(vector_size, 2, name='layer_1')\n","            self.layer_2 = RegualizedLinearLayer(2, 1, name='layer_2')\n","        elif model_name == 'linear':\n","            self.layer_1 = torch.nn.Linear(vector_size, 2)\n","            self.layer_2 = torch.nn.Linear(2, 1)\n","        else:\n","            raise NotImplemented(f'{model_name} is not implemented')\n","\n","    def reset_parameters(self):\n","        self.layer_1.reset_parameters()\n","        self.layer_2.reset_parameters()\n","\n","    def set_tau(self, tau):\n","        if self.model_name == 'GumbelNAC':\n","            self.layer_1.set_tau(tau)\n","            self.layer_2.set_tau(tau)\n","\n","    def regualizer(self):\n","        return self.layer_1.regualizer() + self.layer_2.regualizer()\n","\n","    def set_iteration(self, iteration):\n","        self.layer_1.set_iteration(iteration)\n","        self.layer_2.set_iteration(iteration)\n","\n","    def forward(self, input):\n","        z_1 = self.layer_1(input)\n","        z_2 = self.layer_2(z_1)\n","        return z_2\n","\n","    def extra_repr(self):\n","        return 'vector_size={}'.format(\n","            self.vector_size\n","        )\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" # Training\n",""],"metadata":{}},{"source":["dataset = Dataset(vector_size=6, seed=0)\n","model = Network('GumbelNAC', vector_size=6)\n","model.reset_parameters()\n","\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","\n","for epoch_i in range(0, 100000):\n","    model.set_tau(max(0.1, math.exp(-1e-5 * epoch_i)))\n","    model.set_iteration(epoch_i)\n","\n","    # Prepear\n","    x, t = dataset.batch()\n","    optimizer.zero_grad()\n","\n","    # Loss\n","    y = model(x)\n","    critation_loss = criterion(y, t)\n","    regualizer_loss = 0.1 * model.regualizer()\n","    loss = critation_loss + regualizer_loss\n","\n","    # Optimize\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch_i % 1000 == 0:\n","        print(f'train {epoch_i}: {critation_loss} + {regualizer_loss} = {loss}')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}